{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf5c44b-610b-496a-8734-a3afbbadde75",
   "metadata": {},
   "source": [
    "Initializing and cleaning the [Credit Card Fraud Detection Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?select=creditcard.csv) from download directory that was downloaded from Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7885e52-7f79-4f63-90f9-40c6cddde4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#file path to credit card csv file\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"creditcard.csv\")\n",
    "df = pd.read_csv(file_path) #read csv file as pandas object\n",
    "CC_data = df.to_numpy() #CC_data will contain the Credit Card Fraud detection dataset as a numpy object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f13745-17aa-4ba8-926c-5f28911cba2f",
   "metadata": {},
   "source": [
    "0.0# Agglomerative Hierarchical Clustering\n",
    "\n",
    "<strong>Main Idea:</strong> \n",
    "- Continously merge clusters that are similar to each other\n",
    "- Once there are no clusters that are similar to one another, observe each defined cluster\n",
    "- Clusters with very few number of datapoints are most likely to be deemed an anomaly\n",
    "\n",
    "<strong>Similar Clusters:</strong>\n",
    "- The similarity between clusters will be defined as the distance between clusters\n",
    "- There will be three different different methods of calculating the distance between clusters: Single Linkage, Complete Linkage, Average Linkage\n",
    "- Single Linkage: the distance between two clusters is defined as the smallest distance between two points in each cluster\n",
    "- Complete Linkage: the distance between two clusters is defined as the longest distance between two points in each cluster\n",
    "- Average Linkage: the distance between clusters is defined as the average distance between each point in one cluster to every point in the other cluster\n",
    "- The two clusters with the smallest distance between them will be deemed most similar\n",
    "\n",
    "<strong>Threshold Value:</strong>\n",
    "- Now that we have our most similar clusters, we want to merge them only if the distance between them are less than a given threshold value\n",
    "- We add this condition because eventually, the closest cluster might be an anomaly cluster, so we want to prevent merging with the anomaly cluster\n",
    "- So if the algorithm ever reaches a point where the most similar cluster is a cluster extremely far away(more than our threshold value), we break out of the loop\n",
    "       \n",
    "<strong>Finding Outliers:</strong>\n",
    "- We now have all our clusters\n",
    "- We iterate through our clusters and we know that if cluster sizes are big, then that cluster has homogenous datapoints\n",
    "- So clusters with a small size are more likely to be an anomaly\n",
    "- In our algorithm, we say clusters with size 1 will be deemed an anomaly\n",
    "\n",
    "  \n",
    "  \n",
    "<strong>Pseudocode:</strong><br>\n",
    "Intialize n clusters in C (n is size of training set)<br>\n",
    "while number of clusters is greater than 1:<br>\n",
    ">for each cluster i in C:\n",
    ">>for each cluster in C after current cluster j:\n",
    ">>>find the distance between cluster i and j\n",
    ">>>if distance is smallest we've calculated and is less than the threshold distance:\n",
    ">>>>store the indexes of the cluster  \n",
    "\n",
    ">if no indexes are stored, we know that closest distance does not meet threshold:\n",
    ">>break\n",
    "\n",
    "\n",
    ">merge the two clusters with closest distance\n",
    "\n",
    "cluster sizes that are 1 can be deemed outliers\n",
    "\n",
    "<strong>Hyperparamters:</strong>\n",
    "- Threshold\n",
    "- Distance Metric\n",
    "- Linkage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5821030-2fa2-4aed-8362-9639e93906b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class AgglomerativeCluster:\n",
    "    def __init__(self, data, distance_metric=\"euclidian\", linkage=\"single\", threshold= 5500):\n",
    "        self.distance_metric = distance_metric\n",
    "        self.linkage = linkage\n",
    "        self.data = data\n",
    "        self.threshold=threshold\n",
    "        self.predictions = []\n",
    "    def find_clusters(self):\n",
    "        clusters = [{i} for i in range(len(self.data))] #create one cluster for each datapoint\n",
    "        while len(clusters)>1: # continue running until all datapoints converge into one cluster\n",
    "            min_distance = float('inf') #this will keep track of the distance of the two closest clusters\n",
    "            index_clusters = [0,0] #the indexes of the closest clusters\n",
    "            for i in range(len(clusters)): #iterate through each cluster\n",
    "                for j in range(i+1, len(clusters)):\n",
    "                    #find the the distance between two given clusters\n",
    "                    cur_distance = self.distance_cluster(clusters, clusters[i] , clusters[j])\n",
    "                    #if the distance is less than the minimum distance and it is within the threshold,\n",
    "                    #store the minimum distance and the indices of those clusters\n",
    "                    if cur_distance<min_distance and cur_distance<self.threshold:\n",
    "                        index_clusters[0] = i\n",
    "                        index_clusters[1] = j\n",
    "                        min_distance = cur_distance\n",
    "            #if no indices are stores, that means our minimum distance never was below the threshold, so break\n",
    "            if index_clusters[0] == 0 and index_clusters[1]==0:\n",
    "                break\n",
    "            #merge the two most similar clusters\n",
    "            clusters[index_clusters[0]].update(clusters[index_clusters[1]])\n",
    "            del clusters[index_clusters[1]]\n",
    "        predictions = [] #indices of clusters\n",
    "        for i in range(len(clusters)):\n",
    "            #all clusters that have one datapoint will be considered a cluster\n",
    "            if len(clusters[i])<= 1:\n",
    "                predictions.append(clusters[i])\n",
    "        self.predictions= predictions\n",
    "        return predictions\n",
    "    \n",
    "    def distance_cluster(self, clusters, cluster1, cluster2):\n",
    "        #finds the minimum distance between two clusters\n",
    "        if self.linkage == \"single\":\n",
    "            minDistance = float('inf')\n",
    "            for datapoint1 in cluster1:\n",
    "                for datapoint2 in cluster2:\n",
    "                    curDistance = self.find_distance(datapoint1, datapoint2)\n",
    "                    if curDistance<minDistance:\n",
    "                        minDistance = curDistance\n",
    "            return minDistance\n",
    "        #finds the maximum distance between two clusters\n",
    "        elif self.linkage == \"complete\":\n",
    "            maxDistance = float('-inf')\n",
    "            for datapoint1 in cluster1:\n",
    "                for datapoint2 in cluster2:\n",
    "                    curDistance = self.find_distance(datapoint1, datapoint2)\n",
    "                    if curDistance>maxDistance:\n",
    "                        maxDistance = curDistance\n",
    "            return maxDistance\n",
    "        \n",
    "        #finds the average distance between two clusters\n",
    "        elif self.linkage == \"average\":\n",
    "            totalDistance = 0\n",
    "            for datapoint1 in cluster1:\n",
    "                for datapoint2 in cluster2:\n",
    "                    totalDistance += self.find_distance(datapoint1, datapoint2)\n",
    "            return totalDistance/(len(cluster1)*len(cluster2))\n",
    "        \n",
    "    def find_distance(self, datapoint1, datapoint2):\n",
    "        #calculates euclidian distance between two points\n",
    "        if self.distance_metric == \"euclidian\":\n",
    "            acc = 0\n",
    "            for f in range(len(self.data[datapoint1])):\n",
    "                acc+= (self.data[datapoint2][f]-self.data[datapoint1][f])**2\n",
    "            return math.sqrt(acc)\n",
    "        #calculates chebyshev distance\n",
    "        if self.distance_metric == \"chebyshev\":\n",
    "            acc = 0\n",
    "            for f in range(len(self.data[datapoint1])):\n",
    "                acc = max(acc, abs((self.data[datapoint2][f]-self.data[datapoint1][f])))\n",
    "            return acc\n",
    "        #calculates manhattan distance\n",
    "        if self.distance_metric == \"manhattan\":\n",
    "            for f in range(len(self.data[datapoint1])):\n",
    "                acc+= abs(self.data[datapoint2][f]-self.data[datapoint1][f])\n",
    "            return acc\n",
    "    def recall(self, outliers, predictions):\n",
    "        TruePositive = 0\n",
    "        FalseNegative = 0\n",
    "        for i in outliers:\n",
    "            if i not in predictions:\n",
    "                FalseNegative+=1\n",
    "            if i in predictions:\n",
    "                TruePositive+=1\n",
    "        return TruePositive/(TruePositive+FalseNegative)\n",
    "    def precision(self, outliers, predictions):\n",
    "        TruePositive = 0\n",
    "        FalsePositive = 0\n",
    "        for i in predictions:\n",
    "            if i not in outliers:\n",
    "                FalsePositive+=1\n",
    "            if i in predictions:\n",
    "                TruePositive+=1\n",
    "        return TruePositive/(TruePositive+FalsePositive)\n",
    "    def f1Score(self, outliers, predictions):\n",
    "        recall = self.recall(outliers, predictions)\n",
    "        precision = self.precision(outliers, predictions)\n",
    "        return 2*(precision*recall)/(precision+recall)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff5340-36b2-45d9-90e6-019d20d1930f",
   "metadata": {},
   "source": [
    "<strong>Parallelization:</strong>\n",
    "- Because the Agglomerative Clustering algorithm has a time complexity of O(n^3), we need to parallelize the data so multiple models run at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c94b7fe6-29a4-4659-a9e2-a13da940b901",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             outliers\u001b[38;5;241m.\u001b[39mappend((i,j))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#call find clusters for each model in parallel and store the results in 'results'\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#results will contain indices of the predicted anomaly for each model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallel_method_call\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_models\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(outliers)\n",
      "File \u001b[0;32m/scratch/knerr/venvs/cs66/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/scratch/knerr/venvs/cs66/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/scratch/knerr/venvs/cs66/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#function that will be called in parallel\n",
    "from joblib import Parallel, delayed\n",
    "def parallel_method_call(model):\n",
    "    return model.find_clusters()\n",
    "\n",
    "\n",
    "n_cores = 10  # Use 10 cores\n",
    "chunk_size = 10000 // n_cores #each cores' data sizeAfter identifying the hyperparameters that produce the highest precision score from the Grid Search Function for the given training fold in the pipeline function, the pipeline function will use both the identified hyperparameters and the pipeline training fold to train the Isolation Forest.  The precision of the trained isolation forest is then calculated using the pipeline testing fold. This entire process of finding the best hyperparameters and applying it to the pipeline training \n",
    "all_models = [] #will keep all Agglomerative Clustering Models\n",
    "partitioned_data = [] #will keep track of the partitioned data for each model(matching indices with 'all_models')\n",
    "\n",
    "for i in range(10):\n",
    "    feature_CC = CC_data[np.random.choice(CC_data.shape[0], chunk_size, replace=True)]\n",
    "    original = feature_CC[:]\n",
    "    partitioned_data.append(original)\n",
    "    feature_CC = np.array([arr[:-1] for arr in feature_CC])  \n",
    "    model = AgglomerativeCluster(feature_CC[:], linkage = \"average\", distance_metric=\"chebyshev\")\n",
    "    all_models.append(model)\n",
    "\n",
    "#outliers will contain indices of true outliers of the dataset\n",
    "#so the first indice i will contain which model we are referring to, and j will refer to which indice of the model i's dataset\n",
    "outliers = []\n",
    "for i in range (len(partitioned_data)):\n",
    "    for j in range(len(partitioned_data[i])):\n",
    "        if partitioned_data[i][j][-1]==1:\n",
    "            outliers.append((i,j))\n",
    "#call find clusters for each model in parallel and store the results in 'results'\n",
    "#results will contain indices of the predicted anomaly for each model\n",
    "results = Parallel(n_jobs=-1)(delayed(parallel_method_call)(model) for model in all_models)\n",
    "print(results)\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d31f3e-4524-42d8-a6b2-654374176c62",
   "metadata": {},
   "source": [
    "Calculating Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a55efd9-da70-4cb5-a978-3b5b5d2e6819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "def calc_precision(results, outliers):\n",
    "    acc = 0 \n",
    "    TruePositive = 0\n",
    "    FalsePositive = 0 \n",
    "    for i in range(len(results)):\n",
    "        if not results[i]:\n",
    "            continue\n",
    "        for j in range(len(results[i])):\n",
    "            results[i][j] = list(results[i][j])\n",
    "            if (i, results[i][j][0]) in outliers:\n",
    "                TruePositive+=1\n",
    "            else:\n",
    "                FalsePositive+=1\n",
    "    res = TruePositive/ (TruePositive+FalsePositive) \n",
    "    print(\"Precision: \" + str(res))\n",
    "    \n",
    "    \n",
    "\n",
    "def calc_recall(results, outliers):\n",
    "    acc = 0 \n",
    "    for i in range(len(results)):\n",
    "        if not results[i]:\n",
    "            continue\n",
    "        for j in range(len(results[i])):\n",
    "            results[i][j] = list(results[i][j])\n",
    "            if (i, results[i][j][0]) in outliers:\n",
    "                acc+=1\n",
    "    acc = len(outliers) - acc\n",
    "    res = len(outliers) / (len(outliers)+acc) \n",
    "    print(\"Recall: \" + str(res))    \n",
    "\n",
    "calc_precision(results, outliers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
