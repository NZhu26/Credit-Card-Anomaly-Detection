{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa66b9c7-238a-4c64-b460-f89151ec8d05",
   "metadata": {},
   "source": [
    "Initializing and cleaning the [Credit Card Fraud Detection Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?select=creditcard.csv) from download directory that was downloaded from Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a78fcb-d50c-48e2-a0d4-f64ec5149e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#file path to credit card csv file\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"archive(2)\", \"creditcard.csv\")\n",
    "df = pd.read_csv(file_path) #read csv file as pandas object\n",
    "CC_data = df.to_numpy() #CC_data will contain the Credit Card Fraud detection dataset as a numpy object\n",
    "NaN_instances = []\n",
    "#checking for NaN values in the dataset\n",
    "for instance in CC_data:\n",
    "    for datapoint in instance:\n",
    "        if np.isnan(datapoint):\n",
    "            NaN_instances.append(datapoint)\n",
    "print(NaN_instances) #No NaN values so no need to clean up creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678a97d-73ae-4d09-9057-45cbc5989293",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Local Outlier Factor (LOF)\n",
    "We will be using the local outlier factor algorithm to detect anomalies in our Credit Card Fraud Detection Dataset. To do this, we will be following these steps: \n",
    "<strong>distance:</strong>\n",
    "- Define a Local Outlier Factor class that will hold data about a dataset such as the number of neighbors, distance measurement type, information about the datapoints' neighbors, and an array of local reachability factors for each datapoint. \n",
    "\n",
    "<strong>distance:</strong>\n",
    "- Define a Euclidean distance function to calculate distance for use in KNN algorithm functions.\n",
    "\n",
    "<strong>kNeighbors:</strong>\n",
    "- Gets the k closest neighbors of a query point. Returns a tuple (distance, index) of the k closest neighbors. \n",
    "- The last element in the returned list is the kDistance.\n",
    "\n",
    "<strong>reachabilityDistance:</strong>\n",
    "- Compares the distance between a query point and the comparison point and the kDistance of the comparison point. \n",
    "- The kDistance of the comparison point is the distance between the comparison point and its kth neighbor\n",
    "- Equation<br />\n",
    "\n",
    "Reachability Distance = max(Distance from Point A to Point B, Distance to Kth Neighbor of Point B)\n",
    "\n",
    "<strong>localReachabilityDensity:</strong>\n",
    "- Define a Local Reachability Distance function for our LOF algorithm.\n",
    "- Takes in a list tuples describing the neighbors of a query point that are stuctured like (distanceToNeighbor, index of neighbor). \n",
    "- Calculates the density of local datapoints points based on the distances of the data points surrounding it.\n",
    "- Equation<br />\n",
    "\n",
    "Reachability Density = 1 / Average Reachability Distance of Neighbors of a Point\n",
    "\n",
    "<strong>localOutlierFactor:</strong>\n",
    "- Define a Local Outlier Factor function to find the LOF values for query points. \n",
    "- Equation<br />\n",
    "\n",
    "Local Outlier Factor = Average Reachability Density of Neighbors of a Point / Reachability Density of Point\n",
    "\n",
    "<strong>getChunks:</strong>\n",
    "- Splits up dataset for parallelization\n",
    "\n",
    "<strong>parallel_method_call:</strong>\n",
    "- Method that is called so that the kNeighbors calculations for each datapoint can be done in parallel\n",
    "\n",
    "<strong>createOutlierFactor:</strong>\n",
    "- Utilizes parallelization to find outlier factors for all datapoints. \n",
    "\n",
    "<strong>findThreshold:</strong>\n",
    "- Finds the best Threshold according to the F1 score \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dbd3ee5-dc2b-4e1e-9846-284466441952",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_CC= CC_data[0:1100]\n",
    "fraud_instances = []\n",
    "for i in range(len(feature_CC)):\n",
    "    if feature_CC[i][-1] == 1:\n",
    "        fraud_instances.append(i)\n",
    "feature_CC = np.array([arr[:-1] for arr in feature_CC])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6f81465-33d7-4046-8f8a-5f6e0b9324c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.14947773 -1.12016309 -1.60504217 ... -1.05122482 -1.09622799\n",
      " -1.07488108]\n",
      "[1.14947773 1.12016309 1.60504217 ... 1.05122482 1.09622799 1.07488108]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class Local_Outlier_Factor:\n",
    "    def __init__(self, n_neighbors, metric, datasize):\n",
    "        self.k = n_neighbors\n",
    "        self.distanceType = metric\n",
    "        self.lofScores = []\n",
    "        self.threshold = None\n",
    "        self.neighborInfo = [0] * datasize  # contains list of tuples with information about neighbors for each datapoint [(neigbor1Distance, neighbor1Index), \n",
    "                                            # (neigbor2Distance, neighbor2Index) ...]\n",
    "\n",
    "    def distance(self, a, b):\n",
    "        if self.distanceType == \"euclidean\":\n",
    "            try: \n",
    "                s = 0\n",
    "                if type(a) != type(b) or len(a) != len(b):\n",
    "                    raise Exception(\"The inputs a and b need to be same type and equal length\") \n",
    "                for i in range(len(a)):\n",
    "                    s += (a[i] - b[i])**2\n",
    "                return math.sqrt(s)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e}\")\n",
    "        elif self.distanceType == \"manhattan\":\n",
    "            try: \n",
    "                s = 0\n",
    "                if type(a) != type(b) or len(a) != len(b):\n",
    "                    raise Exception(\"The inputs a and b need to be same type and equal length\") \n",
    "                for i in range(len(a)):\n",
    "                    s += abs(a[i] - b[i])\n",
    "                return s\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e}\")\n",
    "        else:\n",
    "            raise Exception(\"Please input a valid distance type into the LocalOutlierFactor object.\")\n",
    "\n",
    "    def kNeighbors(self, datapoints, query):\n",
    "        neighbors = [0] * (self.k+1)\n",
    "        neighbor = 0\n",
    "        worstBest = float('inf')\n",
    "        for i in range(len(datapoints)):\n",
    "            d = self.distance(datapoints[i], query)\n",
    "            if d < worstBest:\n",
    "                if neighbor < self.k+1:\n",
    "                    neighbors[neighbor] = (d, i)\n",
    "                    neighbor += 1\n",
    "                else:\n",
    "                    maximum = float('-inf')\n",
    "                    maxIndex = 0\n",
    "                    for j in range(len(neighbors)):\n",
    "                        if neighbors[j][0] > maximum:\n",
    "                            maximum = neighbors[j][0]\n",
    "                            maxIndex = j\n",
    "                    worstBest = maximum\n",
    "                    if d < worstBest:\n",
    "                        neighbors[maxIndex] = (d, i)\n",
    "        neighbors.sort()\n",
    "        return neighbors[1:]\n",
    "\n",
    "    def reachabilityDistance(self, distance, kDistance):\n",
    "        return max(distance, kDistance)\n",
    "\n",
    "    def localReachabilityDensity(self, datapoints, neighbors):\n",
    "        foundNeighbors = set()\n",
    "        reachabilityDistanceSum = 0\n",
    "        # Calculates local reachability density\n",
    "        for i in range(len(neighbors)):\n",
    "            distanceToNeighbor = neighbors[i][0]\n",
    "            neighborIndex = neighbors[i][1]\n",
    "            reachabilityDistanceSum += self.reachabilityDistance(distanceToNeighbor, self.neighborInfo[neighborIndex][-1][0])\n",
    "        avgReachabilityDistance = reachabilityDistanceSum / self.k\n",
    "        return 1.0 / (avgReachabilityDistance + 1e-10)\n",
    "\n",
    "    def localOutlierFactor(self, datapoints, query, neighbors):\n",
    "        lrdQuery = self.localReachabilityDensity(datapoints, self.neighborInfo[query])\n",
    "        \n",
    "        lrdNeighborsSum = 0\n",
    "        for i in range(len(neighbors)):\n",
    "            neighborIndex = neighbors[i][1]\n",
    "            lrdNeighborsSum += self.localReachabilityDensity(datapoints, self.neighborInfo[neighborIndex])\n",
    "        avgLRDNeighbors = lrdNeighborsSum / self.k\n",
    "        \n",
    "        LOF = avgLRDNeighbors / lrdQuery\n",
    "        return LOF\n",
    "    \n",
    "    # returns start and end indices of chunks of data\n",
    "    # ex. [[0, 10], [10, 20]] if the number of datapoints is 20 and 2 chunks are needed\n",
    "    def getChunks(self, numDatapoints, numChunks):\n",
    "        chunkSize = numDatapoints//numChunks\n",
    "        res = []\n",
    "        startIndex = 0\n",
    "        if numDatapoints%numChunks == 0: \n",
    "            while startIndex < numDatapoints:\n",
    "                res.append([startIndex, startIndex+chunkSize])\n",
    "                startIndex += chunkSize\n",
    "        else:\n",
    "            while startIndex < numDatapoints-chunkSize:\n",
    "                res.append([startIndex, startIndex+chunkSize])\n",
    "                startIndex += chunkSize\n",
    "            res[-1][1] = numDatapoints\n",
    "        return res\n",
    "        \n",
    "    def parallel_method_call(self, datapoints, indices):\n",
    "        res = []\n",
    "        for i in range(indices[0], indices[1]):\n",
    "            res.append([i, self.kNeighbors(datapoints, datapoints[i])])\n",
    "        return res\n",
    "        \n",
    "    def createOutlierFactor(self, datapoints, cores = 1):\n",
    "        chunkIndices = self.getChunks(len(datapoints), cores) # indices of data for each core\n",
    "        \n",
    "        # Calculates information for each neighbor based on the n_neighbors specified within the Local_Outlier_Factor object.\n",
    "        results = Parallel(n_jobs=-1)(delayed(self.parallel_method_call)(datapoints, chunkIndices[i]) for i in range(len(chunkIndices)))\n",
    "        \n",
    "        for i in range(len(results)):\n",
    "            for j in range(len(results[i])):\n",
    "                self.neighborInfo[results[i][j][0]] = results[i][j][1]\n",
    "        \n",
    "        # Calculates the local reachability factors for each datapoint and puts it within a list\n",
    "        for i in range(len(datapoints)):\n",
    "            self.lofScores.append(self.localOutlierFactor(datapoints, i, self.neighborInfo[i]))\n",
    "        return np.array(self.lofScores)\n",
    "    \n",
    "    def findThreshold(self, trueLabels):\n",
    "        if not self.lofScores:\n",
    "            raise Exception(\"Please calculate local outlier factors first before trying to find a threshold value for them.\")\n",
    "        highestLOF = max(self.lofScores)\n",
    "        curThreshold = min(self.lofScores)\n",
    "        bestThresholdF1 = float('-inf')\n",
    "        bestThreshold = curThreshold\n",
    "        while bestThreshold < highestLOF:\n",
    "            predicted = []\n",
    "            for lof in self.lofScores:\n",
    "                if lof < curThreshold:\n",
    "                    predicted.append(1)\n",
    "                else:\n",
    "                    predicted.append(0)\n",
    "            tn, fp, fn, tp = confusion_matrix(trueLabels, predicted).ravel()\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1Score = 2 * ((precision * recall) / (precision + recall))\n",
    "            if f1Score > bestThresholdF1:\n",
    "                bestThresholdF1 = f1Score\n",
    "                bestThreshold = curThreshold\n",
    "            curThreshold += 0.1\n",
    "        self.threshold = bestThreshold\n",
    "    \n",
    "    def predict(self, datapoints, newData, cores = 1):\n",
    "        if newData:\n",
    "            self.lofScores = []\n",
    "            self.neighborInfo = [0] * len(datapoints)\n",
    "            self.createOutlierFactor(datapoints, cores)\n",
    "            predicted = []\n",
    "            for lof in self.lofScores:\n",
    "                if lof > self.threshold:\n",
    "                    predicted.append(1)\n",
    "                else:\n",
    "                    predicted.append(0)\n",
    "            return predicted\n",
    "        else:\n",
    "            predicted = []\n",
    "            for lof in self.lofScores:\n",
    "                if lof > self.threshold:\n",
    "                    predicted.append(1)\n",
    "                else:\n",
    "                    predicted.append(0)\n",
    "            return predicted\n",
    "        \n",
    "scikitLOF = LocalOutlierFactor(n_neighbors=5, metric=\"euclidean\")\n",
    "scikitLOF.fit_predict(feature_CC)\n",
    "print(scikitLOF.negative_outlier_factor_)\n",
    "\n",
    "myLOF = Local_Outlier_Factor(5, \"euclidean\", len(feature_CC))\n",
    "print(myLOF.createOutlierFactor(feature_CC, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e1ad956-f419-4093-b9c2-b5770c684379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0794735682820022\n",
      "1.0767394204335707\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([myLOF.lofScores[index] for index in fraud_instances]))\n",
    "print(np.mean([myLOF.lofScores[index] for index in range(len(feature_CC))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d1de5a6-8e7f-41ad-80f2-e1e9881e7f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[541, 623]\n",
      "[[1091    7]\n",
      " [   2    0]]\n",
      "True Negative:  1091\n",
      "False Positive:  7\n",
      "False Negative:  2\n",
      "True Positive:  0\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Getting predicted labels\n",
    "y_pred = []\n",
    "for lof in scikitLOF.negative_outlier_factor_:\n",
    "    if lof < -1.7:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "\n",
    "y_true = np.array(df[\"Class\"][0:1100])\n",
    "\n",
    "print(fraud_instances)\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(\"True Negative: \", tn)\n",
    "print(\"False Positive: \", fp)\n",
    "print(\"False Negative: \", fn)\n",
    "print(\"True Positive: \", tp)\n",
    "print(\"Precision: \", tp/(tp+fp))\n",
    "print(\"Recall: \", tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0877a3ec-867a-47cb-a1b6-e5a6b6bf0a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012547051442910915\n",
      "0.9\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# Threshold search\n",
    "threshold = 0.9\n",
    "y_true = np.array(df[\"Class\"][0:1100])\n",
    "maxPrecision = 0\n",
    "maxPrecisionThreshold = 0\n",
    "maxRecall = 0\n",
    "maxRecallThreshold = 0\n",
    "while threshold < 1.5:\n",
    "    y_pred = []\n",
    "    for lof in myLOF.lofScores:\n",
    "        if lof > 1.0:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    if tp/(tp+fp) > maxPrecision:\n",
    "        maxPrecision = tp/(tp+fp)\n",
    "        maxPrecisionThreshold = threshold\n",
    "    if tp/(tp+fn) > maxRecall:\n",
    "        maxRecall = tp/(tp+fn)\n",
    "        maxRecallThreshold = threshold\n",
    "    threshold += 0.1\n",
    "print(maxPrecision)\n",
    "print(maxPrecisionThreshold)\n",
    "print(maxRecall)\n",
    "print(maxRecallThreshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
